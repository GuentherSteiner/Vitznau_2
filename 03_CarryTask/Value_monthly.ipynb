{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The task would be to test a long-only stock-based carry/value strategy.\n",
    "\n",
    "# Carry strategy in stocks would be to long high dividend yield stocks and short the reverse (in this case no need to do short).\n",
    "# Whole value strategy you should be familiar with.\n",
    "\n",
    "# Carry trade in rate hike periods work pretty well (for fx especially). We are interested in whether the same hold for value. \n",
    "# The underlying assets would be S&P 500 stocks.\n",
    "\n",
    "# Please take into account that some companies do stock repurchase instead of cash dividend.\n",
    "# You could try to find adjusted dividend yield data. Time horizon is up to you, but keep in mind potential survivorship bias.\n",
    "# It might be interesting to check the strategy performance during different periods. \n",
    "# Remember to make the backtest a way that they could be implemented (not longing 100 stocks at the same time). \n",
    "# Have a nice night and good luck with your presentation on Monday."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Corinne Vogel\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "C:\\Users\\Corinne Vogel\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "C:\\Users\\Corinne Vogel\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:7: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n",
      "C:\\Users\\Corinne Vogel\\anaconda3\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:7: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import (to_datetime, Int64Index, DatetimeIndex, Period,\n"
     ]
    }
   ],
   "source": [
    "#### Import required Packages ####\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn as sk\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error # to calculate the MSE\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf # To produce ACF plots\n",
    "from statsmodels.graphics.tsaplots import plot_pacf # To produce PACF plots\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose # To decompose Seasons\n",
    "from statsmodels.tsa.stattools import adfuller, kpss # Tests for Stationarity\n",
    "from statsmodels.tsa.ar_model import AutoReg # To produce AR models\n",
    "from statsmodels.stats.anova import anova_lm # To use ANOVA (compare nested models)\n",
    "from statsmodels.tsa.arima.model import ARIMA # To build ARMA & ARIMA Models\n",
    "import statsmodels.stats.diagnostic as dg # To get Breusch-Godfrey Test\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime # to transform variables into datetime objects\n",
    "import math # simple math functions\n",
    "from math import sqrt # square root function\n",
    "import statistics # descriptive statistics library\n",
    "import scipy.stats as stats # descriptive statistics library from scipy\n",
    "import matplotlib.dates as mdates # date formatting\n",
    "from matplotlib.collections import PolyCollection, LineCollection # better plot options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful functions for Matrix calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y): \n",
    "    return x + y                             \n",
    "vecsum = np.vectorize(f)     # Use pd.DataFrame(vecsum(A,B)) to get df of elementwise sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x, y): \n",
    "    return x - y                             \n",
    "vecdif = np.vectorize(g)     # Use pd.DataFrame(vecdif(A,B)) to get df of elementwise differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x, y): \n",
    "    return x * y                             \n",
    "vecmult = np.vectorize(h)    # Use pd.DataFrame(vecmult(A,B)) to get df of elementwise multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i(x, y): \n",
    "    return x / y                             \n",
    "vecdiv = np.vectorize(i)    # Use pd.DataFrame(vecdiv(A,B)) to get df of elementwise division"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotstyle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn plot style ticks to have nicer looking plots\n",
    "sb.set_style(\"ticks\")\n",
    "sb.mpl.rc(\"figure\", figsize=(16,8))\n",
    "sb.mpl.rc(\"font\", size=14)\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Data\n",
    "\n",
    "data_px = pd.read_excel(\"02_Data_clean/SPX_px_cleanest.xlsx\", parse_dates=[\"Date\"])\n",
    "data_px_yly = pd.read_excel(\"02_Data_clean/SPX_px_clean_yearly.xlsx\", parse_dates=[\"Date\"])\n",
    "data_bv = pd.read_excel(\"02_Data_clean/SPX_value_clean.xlsx\", parse_dates=[\"Date\"])\n",
    "\n",
    "bv_resample_fail = pd.read_excel(\"02_Data_clean/SPX_btm_resample_fail_clean.xlsx\")\n",
    "\n",
    "# Check\n",
    "print(type(data_bv))\n",
    "print(data_bv.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample monthly returns\n",
    "data_px.set_index(\"Date\", inplace=True) # set datetimeindex\n",
    "data_bv.set_index(\"Date\", inplace=True) # set datetimeindex\n",
    "data_px_yly.set_index(\"Date\", inplace=True) # set datetimeindex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create monthly returns\n",
    "mtl_ret = data_px.pct_change(fill_method = None).resample(\"M\").agg(lambda x: ((1+x).prod()-1)) # pct_change creates ordinary returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtl_ret = mtl_ret.loc[\"1999-12-31\":\"2022-06-30\", :]\n",
    "mtl_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Book to Market Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide BVPS yearly by SP yearly to get Book to market signal yearly\n",
    "data_btm = pd.DataFrame(vecdiv(data_bv, data_px_yly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Datetimeindex again\n",
    "data_btm.set_index(data_px_yly.index, inplace=True) # set datetimeindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample BTM for monthly rebalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_btm_mt = data_btm.resample(\"M\").ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_btm_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bv_resample_fail.set_index(\"Date\", inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_btm_mt = data_btm_mt.append(bv_resample_fail) # Months until June 2022 added bc resample is a bitch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_btm_mt = pd.concat(\n",
    "    [data_btm_mt, bv_resample_fail],\n",
    "    axis=0,\n",
    "    join=\"outer\",\n",
    "    ignore_index=False,\n",
    "    keys=None,\n",
    "    levels=None,\n",
    "    names=None,\n",
    "    verify_integrity=False,\n",
    "    copy=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_btm_mt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Desired Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The approach is as follows: \n",
    "# 1. Take Book to market value as BVPS(t-6mt) / SP(t-6mt)\n",
    "# 2. for each month calculate the 10 stocks considered for the long strategy\n",
    "# 3. calculate the PF return\n",
    "# 4. take into account the transaction costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(data_btm_mt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty df for weights\n",
    "cols = (data_btm_mt.columns[0:])\n",
    "rows = (data_btm_mt.iloc[:,0])\n",
    "\n",
    "des_weight = pd.DataFrame(index = rows, columns = (cols))\n",
    "des_weight.set_index(data_btm_mt.index, inplace=True) # set datetimeindex\n",
    "des_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_btm_mt.index = data_btm_mt.index.astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_btm_mt.index.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign weights based on 10 largest BTM value\n",
    "for i in data_btm_mt.index:\n",
    "    x = data_btm_mt.loc[i,:].nlargest(10)\n",
    "    for j in data_btm_mt.columns:\n",
    "        if j in x.index:\n",
    "            des_weight.loc[i,j] = 0.1\n",
    "        else:\n",
    "            des_weight.loc[i,j] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_weight.head(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted Returns on 30. June"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut for analysis dataframe\n",
    "mtl_ret = mtl_ret.loc[\"2000-06-30\":\"2022-06-30\", :]\n",
    "des_weight = des_weight.loc[\"2000-06-30\":\"2022-06-30\", :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_ret = pd.DataFrame(vecmult(des_weight, mtl_ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_ret.set_index(mtl_ret.index, inplace=True) # set datetimeindex\n",
    "weighted_ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PF returns weighted per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones500x1 = np.ones(len(weighted_ret.columns))\n",
    "pd.DataFrame(ones500x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_ret_unadj = np.matmul(weighted_ret,ones500x1)\n",
    "pf_ret_unadj = pd.DataFrame(pf_ret_unadj)\n",
    "pf_ret_unadj.columns =[\"PF_returns_unadjusted\"]\n",
    "pf_ret_unadj.loc[\"2000-06-30\"] = 0.0 #bc we do not make returns in first period\n",
    "pf_ret_unadj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1+return -> for stock returns and for the weighted PF returns -> used for actual weights calculation\n",
    "mtl_ret_gross = mtl_ret.transform(lambda x: x + 1)\n",
    "pf_ret_unadj_gross = pf_ret_unadj.transform(lambda x: x + 1)\n",
    "mtl_ret_gross\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create (23x500) matrix to match midyear return matrix for scaling factor to calculate adjusted weights\n",
    "ones_matrix = np.ones((len(pf_ret_unadj_gross),500))\n",
    "pf_ret_unadj_gross_matrix = pd.DataFrame(vecmult(pf_ret_unadj_gross, ones_matrix))\n",
    "pf_ret_unadj_gross_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate weight scaling factor by deviding the returns of the assets, per year by the weighted pf return per year\n",
    "weight_scaleFactor = pd.DataFrame(vecdiv(mtl_ret_gross,pf_ret_unadj_gross_matrix), columns = mtl_ret_gross.columns, index = mtl_ret_gross.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift by -1 to create w(t-1) but last row becomes N/A\n",
    "des_weight_lag1 = des_weight.shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate actual weights before rebalancing\n",
    "actual_weight = pd.DataFrame(vecmult(des_weight_lag1, weight_scaleFactor), columns = weight_scaleFactor.columns, index = weight_scaleFactor.index)\n",
    "actual_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "des_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_weight.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover = pd.DataFrame(abs(vecdif(des_weight, actual_weight)), index = weight_scaleFactor.index)\n",
    "turnover.loc[\"2000-06-30\",:] = des_weight.loc[\"2000-06-30\",:] # Adjust for buying the whole portfolio at the 30.06.2000\n",
    "turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "turnover_mtl = np.matmul(turnover,ones500x1)\n",
    "turnover_mtl = pd.DataFrame(turnover_mtl, columns = [\"Yearly Turnover of PF\"])\n",
    "turnover_mtl.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust Portfolio Return for Transaction cost and turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate transaction costs yearly by multiplying turnover and transaction costs\n",
    "c = np.ones(len(turnover_mtl)) * 0.002 # 0.2% from Michael Milliker for S&P500 stocks -> transaction cost corresponding to turnover in %\n",
    "c = pd.DataFrame(c)\n",
    "tc_mtl = pd.DataFrame(vecmult(turnover_mtl,c), index=turnover_mtl.index, columns= [\"TC_mtl\"])\n",
    "tc_mtl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_ret_unadj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_ret_adj = pd.DataFrame(vecdif(pf_ret_unadj, tc_mtl), index = tc_mtl.index, columns = [\"PF_ret_adj\"])\n",
    "pf_ret_adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_ret_adj_gross = pf_ret_adj.transform(lambda x: x + 1)\n",
    "pf_ret_adj_gross_cum = pf_ret_adj_gross.cumprod() # We don't start at one bc of initial costs for buying pf\n",
    "pf_ret_unadj_gross_cum = pf_ret_unadj_gross.cumprod() # We don't start at one bc of initial costs for buying pf\n",
    "pf_ret_adj_gross_cum.columns = [\"pf_ret_adj_cum\"]\n",
    "pf_ret_adj_gross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(pf_ret_adj_gross_cum.index,pf_ret_adj_gross_cum, label=\"Value Long Leg with TC\", color=\"black\")\n",
    "ax.plot(pf_ret_unadj_gross_cum.index,pf_ret_unadj_gross_cum, label=\"Value Long Leg no TC\", color=\"grey\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_ylabel('Cumulative Returns, indexed 30.06.2000')\n",
    "plt.savefig(\"03_Figures/Value_Performance_monthly_rebalance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(pf_ret_adj.index,pf_ret_adj, label=\"Value Long Leg monthly rebalance with TC\", color=\"black\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_ylabel('Monthly Adj. PF Returns, indexed 30.06.2000')\n",
    "plt.savefig(\"03_Figures/Value_monthly_returns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_ret_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in pf_ret_adj.iloc[:,0] if i > 0.05]\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [i for i in pf_ret_adj.iloc[:,0] if i < -0.05]\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mtl_ret_describe = mtl_ret.describe()\n",
    "# mtl_ret_describe.to_excel(\"mlt_ret_describe.xlsx\")\n",
    "\n",
    "# pf_ret_adj.to_excel(\"pf_ret_adj.xlsx\")\n",
    "\n",
    "# des_weight.columns = [mtl_ret.columns]\n",
    "# des_weight.to_excel(\"des_weights.xlsx\")\n",
    "# mtl_ret.to_excel(\"mtl_ret.xlsx\")\n",
    "\n",
    "dtypes = data_px.dtypes\n",
    "dtypes.to_excel(\"dtypes.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_ret_adj.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_ret_adj.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with Benchmark & FED Fund Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spx = pd.read_excel(\"02_Data_clean/SPX_Index_clean.xlsx\", parse_dates=[\"Date\"])\n",
    "data_fund = pd.read_excel(\"02_Data_clean/FED_Fund_rate_Upper_bound.xlsx\", parse_dates=[\"Date\"])\n",
    "data_spx.set_index(\"Date\", inplace=True) # set datetimeindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spx_gross = data_spx.transform(lambda x: x + 1)\n",
    "spx_gross_cum = spx_gross.cumprod() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spx_gross_cum.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_ret_adj_gross"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_ret_adj_gross_cum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(pf_ret_adj_gross_cum.index,pf_ret_adj_gross_cum, label=\"Value Long Leg with TC\", color=\"black\")\n",
    "ax.plot(spx_gross_cum.index,spx_gross_cum, label=\"SPX Index (Survivors)\", color=\"grey\")\n",
    "ax.legend(loc=\"best\")\n",
    "ax.set_ylabel('Cumulative Returns, indexed 30.06.2000')\n",
    "plt.savefig(\"03_Figures/Value_Performance_mtl_rbl_index_comparison\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa336b74a8cbeead930f17f553be49714fc6c4491fbee50d1179d377ec590ae0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
